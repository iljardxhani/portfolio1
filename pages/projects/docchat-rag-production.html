<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DocChat RAG (Production) | Case Study</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Space+Grotesk:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../../assets/css/main.css" />
  </head>
  <body class="editorial">
    <canvas id="mesh-canvas" aria-hidden="true"></canvas>
    <div class="bg-overlay" aria-hidden="true"></div>

    <header class="site-header">
      <div class="container">
        <a class="brand" href="../../portfolio.html">
          <span class="brand-mark"></span>
          <span>Iljard <strong>Xhani</strong> · AI Engineering</span>
        </a>
        <nav class="nav-links" aria-label="Primary">
          <a href="../../portfolio.html">Home</a>
          <a href="./index.html" aria-current="page">Projects</a>
          <a href="../process.html">Process</a>
          <a href="../about.html">About</a>
          <a href="../contact.html">Contact</a>
        </nav>
        <a class="btn small primary" href="../../Iljard_Xhani_CV.pdf">Download CV</a>
      </div>
    </header>

    <main class="page-main">
      <section class="hero page-hero page-hero-compact">
        <div class="container">
          <div class="hero-panel hero-panel-left reveal">
            <p class="eyebrow">Case Study · RAG</p>
            <h1>DocChat RAG (Production)</h1>
            <p class="lede">
              PDF/doc upload, chunking, embeddings, semantic retrieval, and answer generation with explicit citations
              and source panels. Includes history per user session for follow-up questions.
            </p>
            <div class="quick-points">
              <div class="quick-point"><strong>Problem</strong><span>Answers from internal docs are inconsistent and hard to verify.</span></div>
              <div class="quick-point"><strong>Solution</strong><span>Grounded RAG with citations and source confidence signals.</span></div>
              <div class="quick-point"><strong>Impact</strong><span>Higher trust, lower hallucination risk, faster onboarding answers.</span></div>
            </div>
          </div>
        </div>
      </section>

      <section>
        <div class="container">
          <div class="section-head reveal">
            <p class="section-kicker">Architecture</p>
            <h2>System Design</h2>
            <p>
              Keep the runtime simple for users but strict internally: versioned ingestion, hybrid retrieval,
              citation-aligned generation, and release safety checks.
            </p>
          </div>
          <article class="tech-peek reveal detail-spaced">
            <pre class="code-peek"><code>sources -> ingestion contracts -> chunk/version store
                     |                    |
                     v                    v
               lexical index         vector index
                     \                  /
                      \                /
                      hybrid retrieval + rerank
                               |
                               v
                  citation-constrained answer runtime
                               |
                               v
                 response + citations + source panels</code></pre>
            <p class="peek-why">
              <strong>Why this matters:</strong> this keeps answer quality stable as the corpus and traffic grow.
            </p>
          </article>
          <div class="cards-3 detail-spaced">
            <article class="glass-card reveal"><h3>Ingestion Service</h3><p>Extract text, normalize artifacts, chunk by boundaries, and persist source/version lineage.</p></article>
            <article class="glass-card reveal"><h3>Hybrid Retrieval</h3><p>Dense + lexical retrieval with reranking and confidence filtering before context assembly.</p></article>
            <article class="glass-card reveal"><h3>Answer + Guardrails</h3><p>Citation-constrained generation with fallback behavior when grounding confidence is low.</p></article>
          </div>
        </div>
      </section>

      <section>
        <div class="container">
          <div class="section-head reveal"><p class="section-kicker">Evidence</p><h2>Evaluation and Reliability</h2></div>
          <div class="kpi-grid">
            <article class="glass-card reveal kpi-card"><h3>Faithfulness</h3><p>Tracked against reference answers with citation match checks.</p></article>
            <article class="glass-card reveal kpi-card"><h3>p95 Latency</h3><p>Measured end-to-end from query to sourced answer delivery.</p></article>
            <article class="glass-card reveal kpi-card"><h3>Fallback Rate</h3><p>Monitored when retrieval confidence drops below threshold.</p></article>
          </div>
          <div class="glass-grid detail-spaced">
            <article class="glass-card reveal"><h3>Regression Safety</h3><p>Dataset-based tests run on retrieval and final answer quality before deploy.</p></article>
            <article class="glass-card reveal"><h3>Operational Guardrails</h3><p>Timeout paths, empty-context handling, and structured error envelopes for client apps.</p></article>
          </div>
        </div>
      </section>

      <section>
        <div class="container">
          <div class="section-head reveal">
            <p class="section-kicker">Technical Peek</p>
            <h2>Reliability-Gated Answer Controller</h2>
          </div>
          <article class="tech-peek reveal">
            <pre class="code-peek"><code>def answer_with_policy(env: QueryEnvelope) -> AnswerPayload:
    retrieval = retrieve_context(query=env.query, ...)
    if not retrieval.context:
        return fallback_payload(env=env, reason="empty_context", ...)

    draft = llm.generate(
        prompt=build_prompt_contract(retrieval.query),
        context=retrieval.context,
        history=sessions.get_recent_turns(env.session_id),
    )

    citations = align_citations(draft.answer, retrieval.fused_hits)
    report = build_guardrail_report(
        answer=draft.answer,
        retrieval=retrieval,
        citations=citations,
        policy=policy,
        policy_store=policy_store,
    )

    if report.blocked:
        return fallback_payload(env=env, reason="guardrail_block", ...)

    return AnswerPayload(
        request_id=env.request_id,
        answer=draft.answer,
        citations=citations,
        sources=build_sources(citations),
        confidence=compute_confidence(report, retrieval),
        metrics=collect_runtime_metrics(...),
        guardrails=report,
    )</code></pre>
            <div class="hero-actions peek-actions">
              <button
                class="btn ghost small"
                type="button"
                data-open-code-modal
                data-modal-target="#docchat-code-modal"
                data-snippet-path="../../docs/snippets/docchat_rag_pipeline.py"
              >
                Open Full Code Preview
              </button>
            </div>
            <p class="peek-why">
              <strong>Why this matters:</strong> the answer path is constrained by runtime policy, not just model output.
            </p>
          </article>
        </div>
      </section>

      <section>
        <div class="container">
          <div class="section-head reveal">
            <p class="section-kicker">Advanced Breakdown</p>
            <h2>Most Important Engineering Decisions</h2>
          </div>
          <div class="capability-list">
            <article class="capability-item reveal">
              <div>
                <h3>1. Retrieval Confidence Thresholding</h3>
                <p>
                  Results below relevance policy are excluded; weak evidence routes to fallback instead of forced
                  generation.
                </p>
                <p class="peek-why">
                  <strong>Why this matters:</strong> reduces unsupported answers under difficult queries.
                </p>
              </div>
              <p class="cap-meta">Quality Gate</p>
            </article>
            <article class="capability-item reveal">
              <div>
                <h3>2. Citation-to-Chunk Mapping</h3>
                <p>
                  Every citation token maps to chunk IDs, source title, section, and revision so claims remain
                  auditable.
                </p>
                <p class="peek-why">
                  <strong>Why this matters:</strong> users can verify answers instantly.
                </p>
              </div>
              <p class="cap-meta">Traceability</p>
            </article>
            <article class="capability-item reveal">
              <div>
                <h3>3. Hybrid Dense + Lexical Retrieval</h3>
                <p>
                  Semantic retrieval captures meaning, lexical retrieval catches exact policy terms, reranker merges both
                  signals.
                </p>
                <p class="peek-why">
                  <strong>Why this matters:</strong> improves recall on both narrative and exact-match queries.
                </p>
              </div>
              <p class="cap-meta">Retrieval Quality</p>
            </article>
            <article class="capability-item reveal">
              <div>
                <h3>4. Runtime Guardrails + Fallback Lanes</h3>
                <p>
                  Empty context, low citation precision, and low groundedness each trigger deterministic fallback
                  behavior.
                </p>
                <p class="peek-why">
                  <strong>Why this matters:</strong> failure behavior is predictable in production.
                </p>
              </div>
              <p class="cap-meta">Resilience</p>
            </article>
            <article class="capability-item reveal">
              <div>
                <h3>5. Regression Evaluation Harness</h3>
                <p>
                  Benchmark suites compare current release candidate against baseline before deploy.
                </p>
                <p class="peek-why">
                  <strong>Why this matters:</strong> quality drift is caught before users see it.
                </p>
              </div>
              <p class="cap-meta">Release Safety</p>
            </article>
          </div>
        </div>
      </section>

      <section>
        <div class="container">
          <div class="glass-card reveal">
            <h3>Implementation Notes</h3>
            <div class="token-row">
              <span class="token">FastAPI</span><span class="token">Postgres + pgvector</span><span class="token">Hybrid Retrieval</span><span class="token">Chunk Versioning</span><span class="token">Trace Logging</span>
            </div>
            <p class="path-note">Presentation path: <code>projects/docchat-rag-production/presentations/upcoming/</code></p>
          </div>
        </div>
      </section>
    </main>

    <div
      class="code-modal"
      id="docchat-code-modal"
      role="dialog"
      aria-modal="true"
      aria-hidden="true"
      aria-labelledby="docchat-code-title"
    >
      <div class="code-modal-shell">
        <div class="code-modal-head">
          <h2 id="docchat-code-title">DocChat RAG · Full Production Pipeline Excerpt</h2>
          <button class="btn ghost small" type="button" data-close-code-modal>Close</button>
        </div>
        <p class="code-modal-hint">Read-only preview mode.</p>
        <div class="code-modal-body">
          <pre class="code-peek code-peek-full" data-code-content tabindex="0">Loading code...</pre>
        </div>
      </div>
    </div>

    <footer class="site-footer"><p>© <span id="year">2026</span> Iljard Xhani · AI Application Engineer Portfolio</p></footer>
    <script src="../../assets/js/main.js" defer></script>
  </body>
</html>
